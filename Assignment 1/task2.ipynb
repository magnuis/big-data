{"cells":[{"cell_type":"markdown","source":["#### Names of people in the group\n\nPlease write the names of the people in your group in the next cell."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"83045409-70a5-4010-b1cf-9db8d98f2bad","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Magnus Sagmo\n\nElizabeth Pan"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"059a269d-8f13-494e-84e5-9da5d873047b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n!pip install -q ipython_unittest"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"52939e3d-35c7-4767-a59e-ba75f716e952","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["\u001B[33mWARNING: You are using pip version 21.2.4; however, version 23.0 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-b50ebb6a-8655-4129-976a-f7df2cb51a35/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"]}],"execution_count":0},{"cell_type":"code","source":["# Loading modules that we need\nimport unittest\nfrom pyspark.sql.dataframe import DataFrame\nfrom typing import Any"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"499e46f5-12a4-46de-a212-90f2e936461e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# A helper function to load a table (stored in Parquet format) from DBFS as a Spark DataFrame \ndef load_df(table_name: \"name of the table to load\") -> DataFrame:\n    return spark.read.parquet(table_name)\n\nusers_df = load_df(\"/user/hive/warehouse/users\")\ncomments_df = load_df(\"/user/hive/warehouse/comments\")\nposts_df = load_df(\"/user/hive/warehouse/posts\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7f8c68e0-6bda-40a6-8588-381e51dc0a93","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 1: implenenting two helper functions\nImpelment these two functions:\n1. 'run_query' that gets a Spark SQL query and run it on df which is a Spark DataFrame; it returns the content of the first column of the first row of the DataFrame that is the output of the query;\n2. 'run_query2' that is similar to 'run_query' but instead of one DataFrame gets two; it returns the content of the first column of the first row of the DataFrame that is the output of the query.\n\nNote that the result of a Spark SQL query is itself a Spark DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0a2274f0-1bd8-4812-83d2-f793587e9548","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def run_query(query: \"a SQL query string\", df: \"the DataFrame that the query will be executed on\") -> Any:\n    df.createOrReplaceTempView(\"df\")\n    result = spark.sql(query)\n    return result.collect()[0][0]\n\ndef run_query2(query: \"a SQL query string\", df1: \"DataFrame A\", df2: \"DataFrame B\") -> Any:\n    df1.createOrReplaceTempView(\"df1\")\n    df2.createOrReplaceTempView(\"df2\")\n    result = spark.sql(query)\n    return result.collect()[0][0]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5fce811-7dd2-4602-a243-18a36754c302","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n%load_ext ipython_unittest"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2a021cce-880b-42b1-a09d-6d2c2222124e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 2: writing a few queries\nWrite the following queries in SQL to be executed by Spark in the next cell.\n\n1. 'q1': find the 'Id' of the most recently created post ('df' is 'posts_df') \n2. 'q2': find the number users\n3. 'q3': find the 'Id' of the user who posted most number of answers\n4. 'q4': find the number of questions\n5. 'q5': find the display name of the user who posted most number of comments\n\nNote that 'q1' is already available below as an example. Moreover, remmebr that Spark supports ANSI SQL 2003 so your queries have to comply with that standard."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7de10aba-7e77-4baa-af35-2b4e37916071","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["q1 = \"SELECT * FROM df ORDER BY CreationDate DESC limit 1\"\n\n## YOUR IMPLEMENTATION ##\nq2 = \"SELECT COUNT(DISTINCT Id) FROM df\"\n\n## YOUR IMPLEMENTATION ##\nq3 = \"SELECT OwnerUserId FROM (SELECT OwnerUserId, Count(PostTypeId) as NoOfAnswers from df WHERE PostTypeId = 2 GROUP BY OwnerUserId SORT BY NoOfAnswers DESC) LIMIT 1\"\n\n## YOUR IMPLEMENTATION ##\nq4 = \"SELECT COUNT(Id) FROM df WHERE PostTypeId = 1\"\n\n## YOUR IMPLEMENTATION ##\nq5 = \"\"\"SELECT DisplayName FROM df1 JOIN \n            (SELECT UserID, COUNT(UserId) AS NoOfComments FROM df2 GROUP BY UserId ORDER BY NoOfComments DESC LIMIT 1) \n           ON df1.Id = UserId\"\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c3273ef7-7f4d-4b5d-bcf7-9935c952e26d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 3: validating the implementations by running the tests\n\nRun the cell below and make sure that all the tests run successfully."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46400bac-c11c-4bee-81a8-67d42e3ca968","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%%unittest_main\nclass TestTask2(unittest.TestCase):\n    def test_q1(self):\n        # find the id of the most recent post\n        r = run_query(q1, posts_df)\n        self.assertEqual(r, 95045)\n\n    def test_q2(self):\n        # find the number of the users\n        r = run_query(q2, users_df)\n        self.assertEqual(r, 91616)\n\n    def test_q3(self):\n        # find the user id of the user who posted most number of answers\n        r = run_query(q3, posts_df)\n        self.assertEqual(r, 64377)\n\n    def test_q4(self):\n        # find the number of questions\n        r = run_query(q4, posts_df)\n        self.assertEqual(r, 28950)\n\n    def test_q5(self):\n        # find the display name of the user who posted most number of comments\n        r = run_query2(q5, users_df, comments_df)\n        self.assertEqual(r, \"Neil Slater\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"31ca3fb9-427c-425d-b334-0df9c208a21b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"application/unittest.status+json":{"color":"yellow","message":"","previous":0},"text/plain":""},"removedWidgets":[],"addedWidgets":{},"executionCount":null,"metadata":{"kernelSessionId":"cd758261-b332f64b2d7b39244a64f58b"},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"application/unittest.status+json":{"color":"yellow","message":"","previous":0},"text/plain":""}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"application/unittest.status+json":{"color":"lightgreen","message":".....\n----------------------------------------------------------------------\nRan 5 tests in 6.306s\n\nOK\n","previous":0},"text/plain":"Success"},"removedWidgets":[],"addedWidgets":{},"executionCount":null,"metadata":{"kernelSessionId":"cd758261-b332f64b2d7b39244a64f58b"},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"application/unittest.status+json":{"color":"lightgreen","message":".....\n----------------------------------------------------------------------\nRan 5 tests in 6.306s\n\nOK\n","previous":0},"text/plain":"Success"}},{"output_type":"stream","output_type":"stream","name":"stdout","text":[".....\n----------------------------------------------------------------------\nRan 5 tests in 6.306s\n\nOK\nOut[49]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 4: answering to questions about Spark related concepts\n\nPlease answer the following questions. Write your answer in one to two short paragraphs. Don't copy-paste; instead, write your own understanding.\n\n1. What is the difference between 'DataFrame', 'Dataset', and 'Resilient Distributed Datasets (RDD)'? \n2. When do you suggest using RDDs instead of using DataFrames?\n3. What is the main benefit of using DataSets instead of DataFrames?\n\nWrite your answers in the next cell."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe8a2e0c-7e72-410b-b385-00503c0bc136","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["_1. What is the difference between 'DataFrame', 'Dataset', and 'Resilient Distributed Datasets (RDD)'?_\n\nDataFrames, DataSets and RDDs are all APIs for operating large datasets - they are all immutable, distributed collections of data. \n\nIn _DataFrames,_ the data is organized into named columns, which makes the processing of large datasets easier. \n\nWhere a DataFrame is an untyped set of data, a _DataSet_ the equivalent typed set og data. The typing saves time and cost, as type-errors are caught during complile-time rather than at runtime. \n\nIn _RDDs_ the data is unstructured, and there is no schema that provides a column format. Therefore RDDs are not so optimized for perfomance as the other two alternatives. \n\n[databricks.com](https://www.databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n\n_2. When do you suggest using RDDs instead of using DataFrames?_\n\nI would suggest using RDDs in cases where\n- low-level control is preferred over speed and performance.\n- the data itself is unstructured (such as media streams).\n- you not use domain specific expressions, but rather want to use your own functional programming constructs. \n\n[databricks.com](https://www.databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n\n_3. What is the main benefit of using DataSets instead of DataFrames?_\n\nThe main benefit of using DataSets insted of DataFrames is the type-safety provided at compiler-time. DataSets will notify you during compilation if there are e.g. non-existing column names in the code. If you use DataFrames these errors will not be discovered before the program crashes during runtime - after the data have been (at least partly) loaded."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9c33ca21-1672-4c0c-a876-f73cbb8f65b2","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Task2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3922342795208442}},"nbformat":4,"nbformat_minor":0}
